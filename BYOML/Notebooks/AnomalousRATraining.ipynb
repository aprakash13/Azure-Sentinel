{"cells":[{"cell_type":"markdown","source":["This notebook is used for training anomalous resource access model.  The data used here is File Share Access Events from Windows machine. Data is loaded from a Blob Storage Container.\nThe trained model is then saved to the Blob Storage, which can then be used by the Scoring Notebook\n\nSteps:\n   0. One-time: Install the following packages on the cluster (refer: https://forums.databricks.com/questions/680/how-to-install-python-package-on-spark-cluster.html)\n        - com.microsoft.ml.spark:mmlspark_2.11:1.0.0 from https://mmlspark.azureedge.net\n        - azure_sentinel_utilities whl package\n        - azure-storage-blob (from PyPi - latest based on Azure SDK v12)\n        - plotly (from PyPi)\n        \n   1. One-time: Set credentials in KeyVault so the notebook can access \n        - Storage Account\n   2. Ensure the settings in the first cell below are filled in.\n   3. Run the Notebook to produce the model\n   \n One-time: (Setting up Storage Key in KeyVault)\n    - (Refer:- https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#access-azure-blob-storage-directly)\n \n Storing and retrieving secrets: \n    - Using Azure KeyVault:- https://docs.azuredatabricks.net/user-guide/secrets/secret-scopes.html#akv-ss"],"metadata":{}},{"cell_type":"code","source":["import datetime as dt\n\n# Storage Account Connection String\nstorage_conn_str = dbutils.secrets.get(scope = 'YOUR_SCOPE_HERE', key = 'YOUR_KEY_HERE')\n\n# Workspace Resource Id of your Sentinel workspace\nworkspaceResourceId = 'YOUR_WORKSPACE_RESOURCE_ID_HERE' # eg: /subscriptions/<sub_guid>/resourcegroups/<rg_name>/providers/microsoft.operationalinsights/workspaces/<wks_name>'\nmount_point_name = 'YOUR_MOUNT_POINT_HERE' # any name\n# Project name\nproject = 'YOUR_PROJECT_HERE' # any name\n\n###\n### Note that when training periodically, you specify time range relative to current time as specified in the commented section\n###\n# Time range for training\n# train_start_time = dt.datetime.now() - dt.timedelta(days=20)\n# train_end_time = dt.datetime.now() - dt.timedelta(days=10)\ntrain_start_time = dt.datetime.strptime('2020-09-03 00:00', '%Y-%m-%d %H:%M') \ntrain_end_time = dt.datetime.strptime('2020-09-04 00:00', '%Y-%m-%d %H:%M') \n\nprint (train_start_time)\nprint (train_end_time)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import re\n\n#extract storage account and key from connection string\nkey_pattern = 'DefaultEndpointsProtocol=(\\w+);AccountName=(\\w+);AccountKey=([^;]+);'\nmatch = re.match(key_pattern, storage_conn_str)\nstorage_account = match.group(2)\nstorage_key = match.group(3)\n\nprint (storage_account)\n\ncontainer = 'am-securityevent' # This name is fixed for security events\n\ntrain_base_path = 'WorkspaceResourceId={workspaceResourceId}'.format(workspaceResourceId=workspaceResourceId)\nprint(train_base_path)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["###\n### You can do this one-time in a separate Notebook, so that you don't cause accidental errors in other Notebooks mounting/unmounting the folder\n###\n\n# Mount the Storage Container\n#    (Refer:- https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#mount-azure-blob-storage-containers-with-dbfs)\ndbutils.fs.mount(\n source = \"wasbs://\" + container + \"@\" + storage_account + \".blob.core.windows.net\",\n mount_point = mount_point_name,\n extra_configs = {\"fs.azure.account.key.\" + storage_account + \".blob.core.windows.net\":storage_key})"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql import functions as f, types as t\nfrom pyspark.sql.functions import udf\nfrom mmlspark.cyber.anomaly.collaborative_filtering import AccessAnomaly\nfrom azure_sentinel_utilities.azure_storage import storage_blob_manager"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["checkpoint_dir = mount_point_name + 'cache/{0}/checkpoints'.format(project)\ndbutils.fs.mkdirs(checkpoint_dir)\nspark.sparkContext.setCheckpointDir(checkpoint_dir)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["# Load Dataset"],"metadata":{}},{"cell_type":"code","source":["#\n# This class is used to process 'file share access' related events from Security Events\n#\nclass FileShareDataset:\n  \n    def __init__(self, storage_conn_str):\n        self.storage_conn_str = storage_conn_str\n        self.storage_blob_manager = storage_blob_manager(storage_conn_str)\n\n    @staticmethod\n    def _make_days_delta():\n        @udf('double')\n        def days_delta(d2, d1):\n            return 1.0 + (d2 - d1).days\n        return days_delta\n    \n    # NOTE that there are a lot more fields for security events. Below we are picking up only a subset of fields\n    @staticmethod\n    def _security_event_schema():\n        return t.StructType([\n            t.StructField(name = \"Account\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"ShareName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"ShareLocalPath\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"AccountType\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"Computer\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"EventID\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"EventData\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"NewProcessId\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"NewProcessName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"ParentProcessName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"Process\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"ProcessId\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SourceComputerId\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SourceSystem\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SubjectAccount\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SubjectDomainName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SubjectLogonId\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SubjectUserName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SubjectUserSid\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TargetAccount\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TargetDomainName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TargetLogonId\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TargetUserName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TargetUserSid\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TenantId\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TimeCollected\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TimeGenerated\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TokenElevationType\", dataType = t.StringType(), nullable = True),\n        ])\n\n    # Get file share access data from security events\n    def get_fs_dataset(self, start_time, end_time, container, root):   \n        raw_df = self.storage_blob_manager.get_raw_df(\n                                        start_time, \n                                        end_time, \n                                        container, \n                                        root, \n                                        FileShareDataset._security_event_schema(), \n                                        storage_blob_manager.get_blob_service_client(self.storage_conn_str) )\n        # Get FileShare access events\n        return raw_df.where(\n                    f.col('EventID') == '5140'\n                 ).select (\n                    f.lit('0').alias('tenant_id'),\n                    f.col('TimeGenerated'),\n                    f.to_date(f.col('TimeGenerated').cast('timestamp')).cast('timestamp').alias('Timestamp'), # timestamp is set at day 00:00\n                    f.col('Account').alias('user'),\n                    f.col('ShareName').alias('res'),\n                 )\n    \n    # group the file share access per day and assign an initial likelyhood score\n    def get_processed_fs_dataset(self, start_time, end_time, container, root):\n        dd = FileShareDataset._make_days_delta()\n\n        df_fs = self.get_fs_dataset(start_time, end_time, container, root)\n        \n        # group fileshare access events per day\n        daily_fs_activity = df_fs.groupBy(\n                                'tenant_id',\n                                'Timestamp',\n                                'user',\n                                'res'\n                            ).count()\n        \n        # Calculate an initial likelihood score based on count of events\n        return daily_fs_activity.select(\n            f.col('tenant_id'),\n            f.col('Timestamp').alias('timestamp1'),\n            f.col('Timestamp').alias('timestamp2'),\n            'user',\n            'res',\n            'count'\n        ).groupBy(\n            'tenant_id',\n            'user',\n            'res'\n        ).agg({\n            'timestamp1': 'min',\n            'timestamp2': 'max',\n            'count': 'sum'\n        }).select(\n            f.col('tenant_id'),\n            f.col('min(timestamp1)').alias('min_timestamp'),\n            f.col('max(timestamp2)').alias('max_timestamp'),\n            f.col('user'),\n            f.col('res'),\n            (f.col('sum(count)')/dd(f.col('max(timestamp2)'), f.col('min(timestamp1)'))).alias('likelihood')\n        )"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["def getdataset():\n    return FileShareDataset(storage_conn_str).get_processed_fs_dataset(train_start_time, train_end_time, container, train_base_path)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# load the training data\nptraining = getdataset()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["print(ptraining.first())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["print(ptraining.select('tenant_id').distinct().count())"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["ptraining.describe().show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["# Build Model"],"metadata":{}},{"cell_type":"code","source":["# Model building\naccess_anomaly = AccessAnomaly(\n                    tenantCol='tenant_id',\n                    userCol='user',\n                    resCol='res',\n                    likelihoodCol='likelihood',\n                    maxIter=100\n                    )\naccess_anomaly_model = access_anomaly.fit(ptraining)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["# Save Model"],"metadata":{}},{"cell_type":"code","source":["model_output = '{root}/{project}/model_output'.format(root=mount_point_name + 'models', project=project)\nprint(model_output)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["access_anomaly_model.save(\n    '{model_output}/access_anomaly_model'.format(model_output=model_output)\n)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#unmount blob storage\ndbutils.fs.unmount(mount_point_name)"],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"AnomalousRATraining","notebookId":826778813849944},"nbformat":4,"nbformat_minor":0}
