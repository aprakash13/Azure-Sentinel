{"cells":[{"cell_type":"markdown","source":["This notebook is used for scoring using anomalous resource access model. The model was saved by the training Notebook.  This Notebook runs on a schedule, loads the model and score new events. The data used here is File Share Access Events from Windows machine. Data is loaded from a Blob Storage Container. The top scored results are submitted to Log Analytics.\n\nSteps:\n   0. One-time: Install the following packages on the cluster (refer: https://forums.databricks.com/questions/680/how-to-install-python-package-on-spark-cluster.html)\n        - com.microsoft.ml.spark:mmlspark_2.11:1.0.0 from https://mmlspark.azureedge.net\n        - azure_sentinel_utilities whl package\n        - azure-storage-blob (from PyPi - latest based on Azure SDK v12)\n        - plotly (from PyPi)\n        \n   1. One-time: Set credentials in KeyVault so the notebook can access \n        - Storage Account\n        - Log Analytics\n   2. Ensure the settings in the first cell below are filled in.\n   3. Set the Notebook to run on a schedule to score and submit results to LA.\n   \n One-time: (Setting up Storage Key & Log Analytics Key in KeyVault)\n    - (Refer:- https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#access-azure-blob-storage-directly)\n     \n Storing and retrieving secrets: \n    - Using Azure KeyVault:- https://docs.azuredatabricks.net/user-guide/secrets/secret-scopes.html#akv-ss"],"metadata":{}},{"cell_type":"code","source":["import datetime as dt\n\n# Storage Account Connection String\nstorage_conn_str = dbutils.secrets.get(scope = 'YOUR_SCOPE_HERE', key = 'YOUR_KEY_HERE')\n\n# Workspace Resource Id of your Sentinel workspace\nworkspaceResourceId = 'YOUR_WORKSPACE_RESOURCE_ID_HERE' # eg: /subscriptions/<sub_guid>/resourcegroups/<rg_name>/providers/microsoft.operationalinsights/workspaces/<wks_name>'\nmount_point_name = 'YOUR_MOUNT_POINT_HERE' # any name\n# Project name\nproject = 'YOUR_PROJECT_HERE' # specified in training\n\n#Log Analytics WorkSpace (Sentinel) to write the results\nworkspace_id = 'YOUR_WORKSPACE_ID_HERE' # wks_guid\nworkspace_shared_key = dbutils.secrets.get(scope = 'YOUR_SCOPE_HERE', key = 'YOUR_KEY_HERE')\n\n###\n### Note that when scoring periodically, you specify time range relative to current time as specified in the commented section\n###\n# Time range for training\n# test_start_time = dt.datetime.now() - dt.timedelta(hours=1)\n# test_end_time = dt.datetime.now()\ntest_start_time = dt.datetime.strptime('2020-09-03 00:00', '%Y-%m-%d %H:%M') \ntest_end_time = dt.datetime.strptime('2020-09-04 00:00', '%Y-%m-%d %H:%M') \n\nprint (test_start_time)\nprint (test_end_time)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import re\n\n#extract storage account and key from connection string\nkey_pattern = 'DefaultEndpointsProtocol=(\\w+);AccountName=(\\w+);AccountKey=([^;]+);'\nmatch = re.match(key_pattern, storage_conn_str)\nstorage_account = match.group(2)\nstorage_key = match.group(3)\n\nprint (storage_account)\n\ncontainer = 'am-securityevent' # This name is fixed for security events\n\ntest_base_path = 'WorkspaceResourceId={workspaceResourceId}'.format(workspaceResourceId=workspaceResourceId)\nprint(test_base_path)\n\nmodel_path = '{root}/{project}/model_output'.format(root=mount_point_name + 'models', project=project)\nprint(model_path)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["###\n### You can do this one-time in a separate Notebook, so that you don't cause accidental errors in other Notebooks mounting/unmounting the folder\n###\n\n# Mount the Storage Container\n#    (Refer:- https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#mount-azure-blob-storage-containers-with-dbfs)\ndbutils.fs.mount(\n source = \"wasbs://\" + container + \"@\" + storage_account + \".blob.core.windows.net\",\n mount_point = mount_point_name,\n extra_configs = {\"fs.azure.account.key.\" + storage_account + \".blob.core.windows.net\":storage_key})"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\n\nfrom pyspark.sql import functions as f, types as t\nfrom pyspark.sql.functions import udf\n\n#utils\nfrom azure_sentinel_utilities.azure_storage import storage_blob_manager\nfrom azure_sentinel_utilities.log_analytics import log_analytics_client"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["#Load saved model"],"metadata":{}},{"cell_type":"code","source":["from mmlspark.cyber.anomaly.collaborative_filtering import AccessAnomalyModel\naccess_anomaly_model = AccessAnomalyModel.load(\n    spark, \n    '{model_path}/access_anomaly_model'.format(model_path=model_path)\n)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["# Dataset"],"metadata":{}},{"cell_type":"code","source":["#\n# This class is used to process 'file share access' related events from Security Events\n#\nclass FileShareDataset:\n  \n    def __init__(self, storage_conn_str):\n        self.storage_conn_str = storage_conn_str\n        self.storage_blob_manager = storage_blob_manager(storage_conn_str)\n\n    @staticmethod\n    def _make_days_delta():\n        @udf('double')\n        def days_delta(d2, d1):\n            return 1.0 + (d2 - d1).days\n        return days_delta\n    \n    # NOTE that there are a lot more fields for security events. Below we are picking up only a subset of fields\n    @staticmethod\n    def _security_event_schema():\n        return t.StructType([\n            t.StructField(name = \"Account\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"ShareName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"ShareLocalPath\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"AccountType\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"Computer\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"EventID\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"EventData\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"NewProcessId\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"NewProcessName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"ParentProcessName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"Process\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"ProcessId\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SourceComputerId\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SourceSystem\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SubjectAccount\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SubjectDomainName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SubjectLogonId\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SubjectUserName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"SubjectUserSid\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TargetAccount\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TargetDomainName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TargetLogonId\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TargetUserName\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TargetUserSid\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TenantId\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TimeCollected\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TimeGenerated\", dataType = t.StringType(), nullable = True),\n            t.StructField(name = \"TokenElevationType\", dataType = t.StringType(), nullable = True),\n        ])\n\n    # Get file share access data from security events\n    def get_fs_dataset(self, start_time, end_time, container, root):   \n        raw_df = self.storage_blob_manager.get_raw_df(\n                                        start_time, \n                                        end_time, \n                                        container, \n                                        root, \n                                        FileShareDataset._security_event_schema(), \n                                        storage_blob_manager.get_blob_service_client(self.storage_conn_str) )\n        # Get FileShare access events\n        return raw_df.where(\n                    f.col('EventID') == '5140'\n                 ).select (\n                    f.lit('0').alias('tenant_id'),\n                    f.col('TimeGenerated'),\n                    f.to_date(f.col('TimeGenerated').cast('timestamp')).cast('timestamp').alias('Timestamp'), # timestamp is set at day 00:00\n                    f.col('Account').alias('user'),\n                    f.col('ShareName').alias('res'),\n                 )\n    \n    # group the file share access per day and assign an initial likelyhood score\n    def get_processed_fs_dataset(self, start_time, end_time, container, root):\n        dd = FileShareDataset._make_days_delta()\n\n        df_fs = self.get_fs_dataset(start_time, end_time, container, root)\n        \n        # group fileshare access events per day\n        daily_fs_activity = df_fs.groupBy(\n                                'tenant_id',\n                                'Timestamp',\n                                'user',\n                                'res'\n                            ).count()\n        \n        # Calculate an initial likelihood score based on count of events\n        return daily_fs_activity.select(\n            f.col('tenant_id'),\n            f.col('Timestamp').alias('timestamp1'),\n            f.col('Timestamp').alias('timestamp2'),\n            'user',\n            'res',\n            'count'\n        ).groupBy(\n            'tenant_id',\n            'user',\n            'res'\n        ).agg({\n            'timestamp1': 'min',\n            'timestamp2': 'max',\n            'count': 'sum'\n        }).select(\n            f.col('tenant_id'),\n            f.col('min(timestamp1)').alias('min_timestamp'),\n            f.col('max(timestamp2)').alias('max_timestamp'),\n            f.col('user'),\n            f.col('res'),\n            (f.col('sum(count)')/dd(f.col('max(timestamp2)'), f.col('min(timestamp1)'))).alias('likelihood')\n        )"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["def getdataset():\n    return FileShareDataset(storage_conn_str).get_fs_dataset(test_start_time, test_end_time, container, test_base_path)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["ptesting = getdataset()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["print(ptesting.first())"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["ptesting.describe().show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["# Scoring"],"metadata":{}},{"cell_type":"code","source":["pred_df = access_anomaly_model.transform(ptesting)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["pred_df.first()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["pred_df.select('anomaly_score').describe().show()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# report results"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["full_res_df = pred_df.orderBy(f.desc('anomaly_score')).cache()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["full_res_df.first()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Check score of a simulated anomolous user access\n\n#anomalous_user_access = full_res_df.filter(full_res_df.user.like('Domain_282/User_871048'))\n#display(anomalous_user_access)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["##Filter out commonly seen users (automation account that are known to access File Shares)"],"metadata":{}},{"cell_type":"code","source":["# If there are automation user accounts that access different shares and can cause false positives then filter such users out\nusersToFilter = ['Domain_346/User_870818', 'Domain_348/User_231659']\nfiltered_result = full_res_df.filter(full_res_df.user.isin(*usersToFilter) == False)\nfiltered_result = filtered_result.where(f.col('user').endswith('User_255625') == False) # automation user in all domains\nprint(full_res_df.count())\nprint(filtered_result.count())"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["##Rank top anomalous users"],"metadata":{}},{"cell_type":"code","source":["def print_ratio(df, thr):\n    print('ratio of above {0} items {1}/{2} = {3}%'.format(\n        thr,\n        df.filter(f.col('anomaly_score') > thr).count(),\n        df.count(),\n        100.0*df.filter(f.col('anomaly_score') > thr).count()/df.count()\n    ))\n    \nprint_ratio(full_res_df, 0)\nprint_ratio(full_res_df, 2.5)\nprint_ratio(full_res_df, 5)\nprint_ratio(full_res_df, 7.5)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["display(full_res_df)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["#\n# Select a subset of results to send to Log Analytics\n#\nfrom pyspark.sql.window import Window\n\nw = Window.partitionBy(\n                  'tenant_id',\n                  'user',\n                  'res'\n                ).orderBy(\n                  f.desc('anomaly_score')\n                )\n\n# select values above threshold\nresults_above_threshold = filtered_result.filter(filtered_result.anomaly_score > 0.5)\n\n# get distinct resource/user and corresponding timestamp and highest score\nresults_to_la = results_above_threshold.withColumn(\n                  'index', f.row_number().over(w)\n                  ).orderBy(\n                    f.desc('anomaly_score')\n                  ).select(\n                    'tenant_id',\n                    f.col('user'),\n                    f.col('res'),\n                    'timestamp',\n                    'anomaly_score'\n                  ).where(\n                    'index == 1'\n                  ).limit(25).cache()\n  \ndisplay(results_to_la)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["#Write top anomalous scores to Sentinel"],"metadata":{}},{"cell_type":"code","source":["@udf\ndef escape_str(str):\n  return str.replace('\\\\','\\\\\\\\')\n\ndef send_results_to_log_analytics(df_to_la):\n  # The log type is the name of the event that is being submitted.  This will show up under \"Custom Logs\" as log_type + '_CL'\n  log_type = 'AnomalousResourceAccessResult'\n\n  # concatenate columns to form one json record\n  json_records = df_to_la.withColumn('json_field', f.concat(f.lit('{'), \n                                            f.lit(' \\\"TimeStamp\\\": \\\"'), f.from_unixtime(f.unix_timestamp(f.col(\"timestamp\")), \"y-MM-dd'T'hh:mm:ss.SSS'Z'\"), f.lit('\\\",'),\n                                            f.lit(' \\\"User\\\": \\\"'), escape_str(f.col('user')), f.lit('\\\",'),\n                                            f.lit(' \\\"Resource\\\": \\\"'), escape_str(f.col('res')), f.lit('\\\",'),\n                                            f.lit(' \\\"AnomalyScore\\\":'), f.col('anomaly_score'),\n                                            f.lit('}')\n                                           )                       \n                                         )\n  # combine json record column to create the array\n  json_body = json_records.agg(f.concat_ws(\", \", f.collect_list('json_field')).alias('body'))\n\n  if len(json_body.first()) > 0:\n    json_payload = json_body.first()['body']\n    json_payload = '[' + json_payload + ']'\n\n    payload = json_payload.encode('utf-8') #json.dumps(json_payload)\n    print(payload)\n    return log_analytics_client(workspace_id, workspace_shared_key).post_data(payload, log_type)\n  else:\n    return \"No json data to send to LA\"\n\ncount = results_to_la.count()\nif count > 0:\n  print ('Results count = ', count)\n  result = send_results_to_log_analytics(results_to_la)\n  print(\"Writing to Log Analytics result: \", result)\nelse:\n  print ('No results to send to LA')"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# users that were not in the training set\nnever_seen_users = full_res_df.where(f.col('anomaly_score').isNull()).select(f.col('user')).distinct()\n\nprint('Count never seen users:', never_seen_users.count())\ndisplay(never_seen_users)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["#Display all resource accesses by users with highest anomalous score"],"metadata":{}},{"cell_type":"code","source":["from plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot, offline\nprint (__version__) # requires version >= 1.9.0\n\n# run plotly in offline mode\noffline.init_notebook_mode()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["#Find all server accesses of users with high predicted scores\n# For display, limit to top 25 results\nresults_to_display = results_to_la.orderBy(\n                  f.desc('anomaly_score')\n                ).limit(25).cache()\ninteresting_records = filtered_result.join(results_to_display, ['user'], 'left_semi')\nnon_anomalous_records = interesting_records.join(results_to_display, ['user', 'res'], 'left_anti')\n\ntop_non_anomalous_records = non_anomalous_records.groupBy(\n                          'tenant_id',\n                          'user', \n                          'res'\n                        ).agg(\n                          f.count('*').alias('count'),\n                        ).select(\n                          f.col('tenant_id'),\n                          f.col('user'),\n                          f.col('res'),\n                          'count'\n                        )\n\n#pick only a subset of non-anomalous record for UI\nw = Window.partitionBy(\n                  'tenant_id',\n                  'user',\n                ).orderBy(\n                  f.desc('count')\n                )\n\n# pick top non-anomalous set\ntop_non_anomalous_accesses = top_non_anomalous_records.withColumn(\n                  'index', f.row_number().over(w)\n                  ).orderBy(\n                    f.desc('count')\n                  ).select(\n                    'tenant_id',\n                    f.col('user'),\n                    f.col('res'),\n                    f.col('count')\n                  ).where(\n                    'index in (1,2,3,4,5)'\n                  ).limit(25)\n\n# add back anomalous record\nfileShare_accesses = (top_non_anomalous_accesses\n                          .select('user', 'res', 'count')\n                          .union(results_to_display.select('user', 'res', f.lit(1).alias('count'))).cache())"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# get unique users and file shares\nhigh_scores_df = fileShare_accesses.toPandas()\nunique_arr = np.append(high_scores_df.user.unique(), high_scores_df.res.unique())\n\nunique_df = pd.DataFrame(data = unique_arr, columns = ['name'])\nunique_df['index'] = range(0, len(unique_df.index))\n\n# create index for source & target and color for the normal accesses\nnormal_line_color = 'rgba(211, 211, 211, 0.8)'\nanomolous_color = 'red'\nx = pd.merge(high_scores_df, unique_df, how='left', left_on='user', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'userIndex'})\nall_access_index_df = pd.merge(x, unique_df, how='left', left_on='res', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'resIndex'})\nall_access_index_df['color'] = normal_line_color\n\n# results_to_display index, color and \ny = results_to_display.toPandas().drop(['tenant_id', 'timestamp', 'anomaly_score'], axis=1)\ny = pd.merge(y, unique_df, how='left', left_on='user', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'userIndex'})\nhigh_scores_index_df = pd.merge(y, unique_df, how='left', left_on='res', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'resIndex'})\nhigh_scores_index_df['count'] = 1\nhigh_scores_index_df['color'] = anomolous_color\n\n# substract 1 for the red entries in all_access df\nhsi_df = high_scores_index_df[['user','res', 'count']].rename(columns={'count' : 'hsiCount'})\nall_access_updated_count_df = pd.merge(all_access_index_df, hsi_df, how='left', left_on=['user', 'res'], right_on=['user', 'res'])\nall_access_updated_count_df['count'] = np.where(all_access_updated_count_df['hsiCount']==1, all_access_updated_count_df['count'] - 1, all_access_updated_count_df['count'])\nall_access_updated_count_df = all_access_updated_count_df.loc[all_access_updated_count_df['count'] > 0]\nall_access_updated_count_df = all_access_updated_count_df[['user','res', 'count', 'userIndex', 'resIndex', 'color']]\n\n# combine the two tables\nframes = [all_access_updated_count_df, high_scores_index_df]\ndisplay_df = pd.concat(frames, sort=True)\n# display_df.head()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["data_trace = dict(\n    type='sankey',\n    domain = dict(\n      x =  [0,1],\n      y =  [0,1]\n    ),\n    orientation = \"h\",\n    valueformat = \".0f\",\n    node = dict(\n      pad = 10,\n      thickness = 30,\n      line = dict(\n        color = \"black\",\n        width = 0\n      ),\n      label = unique_df['name'].dropna(axis=0, how='any')\n    ),\n    link = dict(\n      source = display_df['userIndex'].dropna(axis=0, how='any'),\n      target = display_df['resIndex'].dropna(axis=0, how='any'),\n      value = display_df['count'].dropna(axis=0, how='any'),\n      color = display_df['color'].dropna(axis=0, how='any'),\n  )\n)\n\nlayout =  dict(\n    title = \"All resources accessed by users with highest anomalous scores\",\n    height = 772,\n    font = dict(\n      size = 10\n    ),    \n)\n\nfig = dict(data=[data_trace], layout=layout)\n\np = plot(fig, output_type='div')\n\ndisplayHTML(p)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# unmount blob storage\ndbutils.fs.unmount(mount_point_name)"],"metadata":{},"outputs":[],"execution_count":36}],"metadata":{"name":"AnomalousRAScoring","notebookId":826778813849964},"nbformat":4,"nbformat_minor":0}
