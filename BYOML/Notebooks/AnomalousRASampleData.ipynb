{"cells":[{"cell_type":"markdown","source":["This notebook demonstrates the use of Anomalous Resource Access model in Sentinel.  It generates training and testing data, trains the Anomalous Resource Access model and uses it to score the test data.  The top predicted scores are submitted to Sentinel workspace.\n\nSteps:\n   0. One-time: Install the following packages on the cluster (refer: https://forums.databricks.com/questions/680/how-to-install-python-package-on-spark-cluster.html)\n        - com.microsoft.ml.spark:mmlspark_2.11:1.0.0 from https://mmlspark.azureedge.net\n        - azure_sentinel_utilities whl package\n        - plotly (from PyPi)\n        \n   1. One-time: Set credentials in KeyVault so the notebook can access \n        - Log Analytics\n\n Storing and retrieving secrets: \n    - Using Azure KeyVault:- https://docs.azuredatabricks.net/user-guide/secrets/secret-scopes.html#akv-ss"],"metadata":{}},{"cell_type":"markdown","source":["# Initialization"],"metadata":{}},{"cell_type":"code","source":["# Specify the Log Analytics WorkSpaceId (of your Sentinel instance).  The workspacekey should be kept in the KeyVault as the best security practice\n\n#Log Analytics WorkSpace (Sentinel)\nworkspace_id = 'YOUR_WORKSPACE_ID_HERE'\n\n# For the shared key, use either the primary or the secondary key of the workspace\nworkspace_shared_key = dbutils.secrets.get(scope = 'YOUR_SCOPE_HERE', key = 'YOUR_KEY_HERE')"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from mmlspark.cyber.dataset import DataFactory \nfrom mmlspark.cyber.anomaly.collaborative_filtering import AccessAnomaly\n\nfrom pyspark.sql import functions as f, types as t\nimport numpy as np\nimport pandas as pd\n\n#utils\nfrom azure_sentinel_utilities.log_analytics import log_analytics_client"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:27:03.600532Z","start_time":"2019-02-11T12:27:01.472150Z"},"collapsed":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":["spark.sparkContext.setCheckpointDir('dbfs:/checkpoint_path/')"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# The following module generates random training and testing data sets. Two test data sets are generated, one with low anomaly and the other with high anomaly compared to the training\n# data. Note that the data generated has a 'score' field that is a seed value for training. When working with real data, you will have a timestamp that you will need to use to\n# calculate a score based on the aggregation of access over a time interval (hourly or daily).\n\nfactory = DataFactory(\n  num_hr_users = 25,\n  num_hr_resources = 50,\n  num_fin_users = 35,\n  num_fin_resources = 75,\n  num_eng_users = 15,\n  num_eng_resources = 25,\n  single_component = True\n)\n\ntraining_pdf = factory.create_clustered_training_data(ratio=0.4)\n\ntraining_df = spark.createDataFrame(training_pdf)\ningroup_df = spark.createDataFrame(factory.create_clustered_intra_test_data(training_pdf))\noutgroup_df = spark.createDataFrame(factory.create_clustered_inter_test_data())"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# This module does 2 things:\n#  a) Gives a default tenant ID of 0\n#  b) Adds a count for number of access (for displaying the graph)\ndef updateValues(df, use_random_count):\n  tmp_df = df.withColumn('tenant_id', f.lit(0))\n  if use_random_count:\n     return tmp_df.withColumn('count_', f.round(1+f.rand()*10))\n  else:\n     return tmp_df.withColumn('count_', f.lit(1))\n\n\ntraining_df = updateValues(training_df, True)\ningroup_df = updateValues(ingroup_df, False)\noutgroup_df = updateValues(outgroup_df, False)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["training_df.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Stats of each dataframes\nprint(training_df.count())\nprint(ingroup_df.count())\nprint(outgroup_df.count())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["training_df.describe().show()"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:33:33.947460Z","start_time":"2019-02-11T12:33:31.368658Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["# Training"],"metadata":{}},{"cell_type":"code","source":["access_anomaly = AccessAnomaly(\n  tenantCol='tenant_id',\n  userCol='user',\n  resCol='res',\n  likelihoodCol='likelihood',\n  maxIter=1000\n)"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:47:42.112994Z","start_time":"2019-02-11T12:40:47.542426Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":["model = access_anomaly.fit(training_df)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["# Testing"],"metadata":{}},{"cell_type":"code","source":["# Score the low anomaly test dataset\ningroup_scored_df = model.transform(ingroup_df)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["ingroup_scored_df.agg(\n  f.min('anomaly_score').alias('min_anomaly_score'),\n  f.max('anomaly_score').alias('max_anomaly_score'),\n  f.mean('anomaly_score').alias('mean_anomaly_score'),\n  f.stddev('anomaly_score').alias('stddev_anomaly_score'),\n).show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Score the high anomaly test dataset\noutgroup_scored_df = model.transform(outgroup_df)"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:47:52.432190Z","start_time":"2019-02-11T12:47:42.116353Z"},"collapsed":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":["outgroup_scored_df.agg(\n  f.min('anomaly_score').alias('min_anomaly_score'),\n  f.max('anomaly_score').alias('max_anomaly_score'),\n  f.mean('anomaly_score').alias('mean_anomaly_score'),\n  f.stddev('anomaly_score').alias('stddev_anomaly_score'),\n).show()"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:47:54.365653Z","start_time":"2019-02-11T12:47:52.981448Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Report results"],"metadata":{}},{"cell_type":"code","source":["full_res_df = outgroup_scored_df.orderBy(f.desc('anomaly_score')).cache()"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:48:01.733431Z","start_time":"2019-02-11T12:48:01.723119Z"},"collapsed":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(full_res_df)"],"metadata":{"ExecuteTime":{"end_time":"2019-02-11T12:48:02.547892Z","start_time":"2019-02-11T12:48:01.736469Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":["def print_ratio(df, thr):\n    print('ratio of above {0} items {1}/{2} = {3}%'.format(\n        thr,\n        df.filter(f.col('anomaly_score') > thr).count(),\n        df.count(),\n        100.0*df.filter(f.col('anomaly_score') > thr).count()/df.count()\n    ))\n    \nprint_ratio(full_res_df, 0)\nprint_ratio(full_res_df, 15.0)\nprint_ratio(full_res_df, 15.5)\nprint_ratio(full_res_df, 15.7)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["#Rank anomalous users"],"metadata":{}},{"cell_type":"code","source":["#\n# Select a subset of results to send to Log Analytics\n#\nfrom pyspark.sql.window import Window\n\nw = Window.partitionBy(\n                  'tenant_id',\n                  'user',\n                  'res'  \n                ).orderBy(\n                  f.desc('anomaly_score')\n                )\n\n# select values above threshold\nresults_above_threshold = full_res_df.filter(full_res_df.anomaly_score > 1.0)\n\n# get distinct resource/user and corresponding timestamp and highest score\nresults_to_la = results_above_threshold.withColumn(\n                  'index', f.row_number().over(w)\n                  ).orderBy(\n                    f.desc('anomaly_score')\n                  ).select(\n                    'tenant_id',\n                    f.col('user'),\n                    f.col('res'),\n                    'anomaly_score'\n                  ).where(\n                    'index == 1'\n                  ).limit(100).cache()\n\n# add a fake timestamp to the results\nresults_to_la = results_to_la.withColumn('timestamp', f.current_timestamp())\n  \ndisplay(results_to_la)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["#Display all resource accesses by users with highest anomalous score"],"metadata":{}},{"cell_type":"code","source":["from plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot, offline\nprint (__version__) # requires version >= 1.9.0\n\n# run plotly in offline mode\noffline.init_notebook_mode()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["#Find all server accesses of users with high predicted scores\n# For display, limit to top 25 results\nresults_to_display = results_to_la.orderBy(\n                  f.desc('anomaly_score')\n                ).limit(25).cache()\ninteresting_records = full_res_df.join(results_to_display, ['user'], 'left_semi')\nnon_anomalous_records = interesting_records.join(results_to_display, ['user', 'res'], 'left_anti')\n\ntop_non_anomalous_records = non_anomalous_records.groupBy(\n                          'tenant_id',\n                          'user', \n                          'res'\n                        ).agg(\n                          f.count('*').alias('count'),\n                        ).select(\n                          f.col('tenant_id'),\n                          f.col('user'),\n                          f.col('res'),\n                          'count'\n                        )\n\n#pick only a subset of non-anomalous record for UI\nw = Window.partitionBy(\n                  'tenant_id',\n                  'user',\n                ).orderBy(\n                  f.desc('count')\n                )\n\n# pick top non-anomalous set\ntop_non_anomalous_accesses = top_non_anomalous_records.withColumn(\n                  'index', f.row_number().over(w)\n                  ).orderBy(\n                    f.desc('count')\n                  ).select(\n                    'tenant_id',\n                    f.col('user'),\n                    f.col('res'),\n                    f.col('count')\n                  ).where(\n                    'index in (1,2,3,4,5)'\n                  ).limit(25)\n\n# add back anomalous record\nfileShare_accesses = (top_non_anomalous_accesses\n                          .select('user', 'res', 'count')\n                          .union(results_to_display.select('user', 'res', f.lit(1).alias('count'))).cache())"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# get unique users and file shares\nhigh_scores_df = fileShare_accesses.toPandas()\nunique_arr = np.append(high_scores_df.user.unique(), high_scores_df.res.unique())\n\nunique_df = pd.DataFrame(data = unique_arr, columns = ['name'])\nunique_df['index'] = range(0, len(unique_df.index))\n\n# create index for source & target and color for the normal accesses\nnormal_line_color = 'rgba(211, 211, 211, 0.8)'\nanomolous_color = 'red'\nx = pd.merge(high_scores_df, unique_df, how='left', left_on='user', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'userIndex'})\nall_access_index_df = pd.merge(x, unique_df, how='left', left_on='res', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'resIndex'})\nall_access_index_df['color'] = normal_line_color\n\n# results_to_display index, color and \ny = results_to_display.toPandas().drop(['tenant_id', 'timestamp', 'anomaly_score'], axis=1)\ny = pd.merge(y, unique_df, how='left', left_on='user', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'userIndex'})\nhigh_scores_index_df = pd.merge(y, unique_df, how='left', left_on='res', right_on='name').drop(['name'], axis=1).rename(columns={'index' : 'resIndex'})\nhigh_scores_index_df['count'] = 1\nhigh_scores_index_df['color'] = anomolous_color\n\n# substract 1 for the red entries in all_access df\nhsi_df = high_scores_index_df[['user','res', 'count']].rename(columns={'count' : 'hsiCount'})\nall_access_updated_count_df = pd.merge(all_access_index_df, hsi_df, how='left', left_on=['user', 'res'], right_on=['user', 'res'])\nall_access_updated_count_df['count'] = np.where(all_access_updated_count_df['hsiCount']==1, all_access_updated_count_df['count'] - 1, all_access_updated_count_df['count'])\nall_access_updated_count_df = all_access_updated_count_df.loc[all_access_updated_count_df['count'] > 0]\nall_access_updated_count_df = all_access_updated_count_df[['user','res', 'count', 'userIndex', 'resIndex', 'color']]\n\n# combine the two tables\nframes = [all_access_updated_count_df, high_scores_index_df]\ndisplay_df = pd.concat(frames, sort=True)\n# display_df.head()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["data_trace = dict(\n    type='sankey',\n    domain = dict(\n      x =  [0,1],\n      y =  [0,1]\n    ),\n    orientation = \"h\",\n    valueformat = \".0f\",\n    node = dict(\n      pad = 10,\n      thickness = 30,\n      line = dict(\n        color = \"black\",\n        width = 0\n      ),\n      label = unique_df['name'].dropna(axis=0, how='any')\n    ),\n    link = dict(\n      source = display_df['userIndex'].dropna(axis=0, how='any'),\n      target = display_df['resIndex'].dropna(axis=0, how='any'),\n      value = display_df['count'].dropna(axis=0, how='any'),\n      color = display_df['color'].dropna(axis=0, how='any'),\n  )\n)\n\nlayout =  dict(\n    title = \"All resources accessed by users with highest anomalous scores\",\n    height = 772,\n    font = dict(\n      size = 10\n    ),    \n)\n\nfig = dict(data=[data_trace], layout=layout)\n\np = plot(fig, output_type='div')\n\ndisplayHTML(p)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["@udf\ndef escape_str(str):\n  return str.replace('\\\\','\\\\\\\\')\n\ndef send_results_to_log_analytics(df_to_la):\n  # The log type is the name of the event that is being submitted.  This will show up under \"Custom Logs\" as log_type + '_CL'\n  log_type = 'AnomalousResourceAccessResult'\n\n  # concatenate columns to form one json record\n  json_records = df_to_la.withColumn('json_field', f.concat(f.lit('{'), \n                                            f.lit(' \\\"TimeStamp\\\": \\\"'), f.from_unixtime(f.unix_timestamp(f.col(\"timestamp\")), \"y-MM-dd'T'hh:mm:ss.SSS'Z'\"), f.lit('\\\",'),\n                                            f.lit(' \\\"User\\\": \\\"'), escape_str(f.col('user')), f.lit('\\\",'),\n                                            f.lit(' \\\"Resource\\\": \\\"'), escape_str(f.col('res')), f.lit('\\\",'),\n                                            f.lit(' \\\"AnomalyScore\\\":'), f.col('anomaly_score'),\n                                            f.lit('}')\n                                           )                       \n                                         )\n  # combine json record column to create the array\n  json_body = json_records.agg(f.concat_ws(\", \", f.collect_list('json_field')).alias('body'))\n\n  if len(json_body.first()) > 0:\n    json_payload = json_body.first()['body']\n    json_payload = '[' + json_payload + ']'\n\n    payload = json_payload.encode('utf-8') #json.dumps(json_payload)\n    # print(payload)\n    return log_analytics_client(workspace_id, workspace_shared_key).post_data(payload, log_type)\n  else:\n    return \"No json data to send to LA\"\n\ncount = results_to_la.count()\nif count > 0:\n  print ('Results count = ', count)\n  result = send_results_to_log_analytics(results_to_la)\n  print(\"Writing to Log Analytics result: \", result)\nelse:\n  print ('No results to send to LA')"],"metadata":{},"outputs":[],"execution_count":30}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"AnomalousRASampleData","notebookId":1322118319405291,"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"anaconda-cloud":{},"toc":{"title_sidebar":"Contents","nav_menu":{},"sideBar":true,"number_sections":true,"skip_h1_title":false,"base_numbering":1,"toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false,"title_cell":"Table of Contents"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":0}
